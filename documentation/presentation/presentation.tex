% @Author: Ramiro Luiz Nunes
% @Date:   2024-05-12 16:28:49
% @Last Modified by:   Ramiro Luiz Nunes
% @Last Modified time: 2024-05-13 20:51:02


%\documentclass{beamer} %voce pode usar este modelo tambem
\documentclass[handout,aspectratio = 169]{beamer}
\usefonttheme{serif}
\usepackage{graphicx,url}
\usepackage[brazil]{babel}   
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{outlines}
\usepackage{comment}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{subfig}
\usepackage{graphicx,url}
\usepackage{subfigure}


% \usepackage{fontspec}
% \setmainfont{Times}
%Image package

\usepackage{animate}
% \usepackage{hyphenat}
% \usepackage{multicol}
% \usepackage{pxfonts}

\batchmode
% \usepackage{pgfpages}
% \pgfpagesuselayout{4 on 1}[letterpaper,landscape,border shrink=5mm]
\usepackage{amsmath,amssymb,enumerate,epsfig,bbm,calc,color,ifthen,capt-of}
\usetheme{Berlin}
\usecolortheme{orchid}

\setbeamertemplate{caption}[numbered]
\setbeamertemplate{footline}[frame number]

%-------------------------Titulo/Autores/Orientador------------------------------------------------
\title{Árvores de Regressão: Processo de Treinamento}

\date{14 de maio de 2024}

\author[]{Introdução ao Aprendizado de Máquina
    \newline Jean Marcelo Mira Junior
    \newline Ramiro Luiz Nunes
}

%-------------------------Logo na parte de baixo do slide------------------------------------------
\pgfdeclareimage[height=1.0cm]{brasao}{brasao.pdf}
\logo{\pgfuseimage{brasao}\hspace*{0.5cm}}

\begin{document}
% -----------------------------------------------------------------------------

%---Gerador de Sumário---------------------------------------------------------
\frame{\titlepage}
\section[]{}
\begin{frame}{Sumário}
  \tableofcontents
\end{frame}
%---Fim do Sumário------------------------------------------------------------

%------------------------------------------------------------------------------

\section{Introdução}

\begin{frame}{Base de Científica}
    \centering % Este comando centraliza todo o conteúdo dentro do frame
    \begin{minipage}[c]{0.3\textwidth}
        \includegraphics[width=\linewidth]{Figs/bib.jpg} % Ajuste o caminho conforme necessário
    \end{minipage}
    \begin{minipage}[c]{0.6\textwidth} % Aumentada a largura para dar mais espaço para o texto
        \centering % Centraliza o conteúdo no slide
        \vspace*{\fill} % Adiciona espaço verticalmente para empurrar o conteúdo para baixo
        \Large{\textbf{Classification and Regression Trees}} % Título do livro em negrito e tamanho grande
        \vspace{1em} % Espaço vertical entre o título e os autores
        
        \small{Leo Breiman, Jerome H. Friedman, Richard A. Olshen, Charles J. Stone} % Autores em tamanho pequeno
        \vspace*{\fill} % Adiciona espaço no final para centralizar verticalmente
    \end{minipage}
\end{frame}

\begin{frame}{Diferença entre Árvores de Decisão e Regressão}
\begin{itemize}
    \item \textbf{Árvores de Decisão:}
    \begin{itemize}
        \item \textit{Objetivo:} Prever categorias (ex: aprovado/reprovado).
        \item \textit{Funcionamento:} Modelo em forma de árvore para decisões recursivas.
        \item \textit{Vantagens:} Fácil interpretação, robustez a outliers.
        \item \textit{Desvantagens:} Risco de sobreajuste, menos eficaz em grandes dados.
    \end{itemize} 
    \vspace{1em} % Espaço vertical de uma linha entre os itens principais
    \item \textbf{Árvores de Regressão:}
    \begin{itemize}
        \item \textit{Objetivo:} Prever valores contínuos (ex: preço de casa).
        \item \textit{Funcionamento:} Usa funções matemáticas para ajustar dados.
        \item \textit{Vantagens:} Alta precisão, útil em relações complexas.
        \item \textit{Desvantagens:} Modelo complexo, sensível a outliers.
    \end{itemize}
\end{itemize}

\end{frame}

\begin{frame}{Diferença entre AID e CART}
\begin{itemize}
    \item \textbf{ AID (Detecção Automática de Interação):}
    \begin{itemize}
        \item Um programa precursor na estrutura de árvore para regressão.
        \item Foco na construção de uma "árvore honesta" através de um processo específico de poda e estimativa.
    \end{itemize}
     \vspace{1em} % Espaço vertical de uma linha entre os itens principais
    \item  \textbf{CART (Árvores de Classificação e Regressão):}
    \begin{itemize}
        \item Uma abordagem mais avançada que se baseia no AID.
        \item Compartilha algumas funcionalidades com o AID, como combinações de variáveis e tratamento de dados ausentes, mas também oferece recursos adicionais:
        \begin{itemize}
            \item Sem restrições no número de valores de variáveis (diferente do AID).
            \item Subamostragem para melhor generalização.
            \item Critérios de divisão diferentes dos usados no AID.
        \end{itemize}
    \end{itemize}
\end{itemize}
\end{frame}

%-----------------------------------------------------------------------------

\section{Construção da Árvore}

\begin{frame}{Construção dos Ramos}
\begin{itemize}
    \item \textbf{Processo de construção envolve}:
    \begin{itemize}
        \item \textit{Definição e Coleta de Dados:} Identificar o problema, coletar e preparar dados.
        \item \textit{Seleção do Algoritmo de Divisão:} Escolher critérios (Gini, entropia) e estratégia de divisão (binária, multivariada).
        \item \textit{Crescimento da Árvore:} Aplicar critérios de divisão, criar ramos e repetir até atingir critérios de parada.
        \item \textit{Poda da Árvore:} Podar para evitar sobreajuste, utilizando técnicas pré e pós-pruna.
        \item \textit{Avaliação e Seleção do Modelo:} Usar validação cruzada para testar desempenho e selecionar o melhor modelo.
    \end{itemize}
\end{itemize}
\end{frame}

%-----------------------------------------------------------------------------

\begin{frame}{Critério de Divisão}
\begin{itemize}
    \item \textbf{Critérios e Considerações}
    \begin{itemize}
        \item \textit{Critérios:}
        \begin{itemize}
            \item \textbf{MSE (Erro Quadrático Médio)}: Precisão de previsão em regressão.
            \item \textbf{Ganho de Informação}: Discriminação em classificação baseada em entropia ou Gini.
            \item \textbf{Entropia e Índice de Gini}: Medem a pureza; baixos valores indicam alta pureza.
        \end{itemize}
        \vspace{1em} % Espaço vertical de uma linha entre os itens principais
        \item \textit{Fatores a Considerar:}
        \begin{itemize}
            \item \textbf{Tipo de Problema}: Gini e entropia para classificação; MSE para regressão.
            \item \textbf{Características dos Dados}: Distribuição de classes e presença de outliers.
            \item \textbf{Complexidade do Modelo}: Ganho de informação aumenta complexidade; MSE simplifica.
        \end{itemize}
    \end{itemize}
\end{itemize}
\end{frame}

%-----------------------------------------------------------------------------

% Slide sobre MSE
\begin{frame}{Métricas de Avaliação}
\begin{itemize}
    \item \textbf{MSE (Erro Quadrático Médio)}
    \begin{itemize}
        \item \textbf{Objetivo}: Avaliar a qualidade de modelos de regressão penalizando grandes erros de previsão.
        \item \textbf{Funcionamento}: Calcula a média dos quadrados das diferenças entre os valores reais e previstos.
        \item \textbf{Vantagens}:
        \begin{itemize}
            \item Penaliza severamente grandes erros.
            \item Utilizável em técnicas de otimização como gradiente descendente.
        \end{itemize}
        \item \textbf{Desvantagens}:
        \begin{itemize}
            \item Alta sensibilidade a outliers.
            \item Difícil interpretação direta.
        \end{itemize}
    \end{itemize}
\end{itemize}
\end{frame}

% Fórmula do MSE com descrições detalhadas
\begin{frame}{Fórmula do MSE}
\begin{equation}
\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
\end{equation}
onde:
\begin{itemize}
    \item \( n \) é o número total de observações.
    \item \( y_i \) é o valor real da \( i \)-ésima observação.
    \item \( \hat{y}_i \) é o valor previsto para a \( i \)-ésima observação.
\end{itemize}
\end{frame}

% Exemplo Prático do MSE
\begin{frame}{Exemplo Prático do MSE}
\begin{itemize}
    \item Valores Reais: [6.575, 6.421, 7.185, 6.998, 7.147]
    \item Valores Previstos: [6.1, 6.0, 7.2, 6.5, 6.3]
    \item Diferenças Quadráticas:
    \begin{itemize}
        \item (6.575 - 6.1)\(^2\) = 0.225625
        \item (6.421 - 6.0)\(^2\) = 0.177241
        \item (7.185 - 7.2)\(^2\) = 0.000225
        \item (6.998 - 6.5)\(^2\) = 0.248004
        \item (7.147 - 6.3)\(^2\) = 0.717409
    \end{itemize}
    \item Soma das Diferenças Quadráticas: 1.368504
    \item MSE = 1.368504 / 5 = 0.273701
\end{itemize}
\end{frame}

%-----------------------------------------------------------------------------

% Slide sobre MAE
\begin{frame}{Métricas de Avaliação}
\begin{itemize}
    \item \textbf{MAE (Erro Absoluto Médio)}
    \begin{itemize}
        \item \textbf{Objetivo}: Avaliar a qualidade de modelos de regressão, medindo o erro médio em termos absolutos.
        \item \textbf{Funcionamento}: Calcula a média dos valores absolutos das diferenças entre os valores reais e previstos.
        \item \textbf{Vantagens}:
        \begin{itemize}
            \item Menos sensível a outliers, oferecendo uma visão mais robusta do desempenho.
            \item Fácil interpretação em termos dos valores originais dos dados.
        \end{itemize}
        \item \textbf{Desvantagens}:
        \begin{itemize}
            \item Não penaliza severamente grandes erros, o que pode levar a ajustes menos precisos em alguns casos.
        \end{itemize}
    \end{itemize}
\end{itemize}
\end{frame}

% Fórmula do MAE com descrições detalhadas
\begin{frame}{Fórmula do MAE}
\begin{equation}
\text{MAE} = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|
\end{equation}
onde:
\begin{itemize}
    \item \( n \) é o número total de observações.
    \item \( y_i \) é o valor real da \( i \)-ésima observação.
    \item \( \hat{y}_i \) é o valor previsto para a \( i \)-ésima observação.
\end{itemize}
\end{frame}

% Exemplo Prático do MAE
\begin{frame}{Exemplo Prático do MAE}
\begin{itemize}
    \item Valores Reais: [6.575, 6.421, 7.185, 6.998, 7.147]
    \item Valores Previstos: [6.1, 6.0, 7.2, 6.5, 6.3]
    \item Diferenças Absolutas:
    \begin{itemize}
        \item |6.575 - 6.1| = 0.475
        \item |6.421 - 6.0| = 0.421
        \item |7.185 - 7.2| = 0.015
        \item |6.998 - 6.5| = 0.498
        \item |7.147 - 6.3| = 0.847
    \end{itemize}
    \item Soma das Diferenças Absolutas: 2.256
    \item MAE = 2.256 / 5 = 0.4512
\end{itemize}
\end{frame}


%-----------------------------------------------------------------------------

\begin{frame}{Métodos de Podagem}
\begin{itemize}
    \item \textbf{O que é Poda?}
    \begin{itemize}
        \item Processo de remover ramos desnecessários para simplificar a árvore e melhorar a generalização do modelo.
    \end{itemize}

    \item \textbf{Tipos de Poda:}
    \begin{itemize}
        \item \textbf{Pré-Poda:} Evita ramos excessivos durante a construção, definindo critérios como profundidade máxima, número mínimo de amostras e valor mínimo de impureza.
        \item \textbf{Pós-Poda:} Remove ramos após a construção da árvore, utilizando validação cruzada e critérios de custo-complexidade.
    \end{itemize}

    \item \textbf{Métodos Populares de Pós-Poda:}
    \begin{itemize}
        \item Redução de Erro e Poda Pessimista: Removem ramos que aumentam o erro de validação.
        \item Poda Baseada em Custo-Complexidade: Elimina ramos que elevam a complexidade sem aumentar a precisão.
    \end{itemize}
\end{itemize}
\end{frame}

%-----------------------------------------------------------------------------

\section{Avaliação da Árvore de Regressão}

\begin{frame}{Texto Aqui}
\end{frame}

\begin{frame}{Texto Aqui}
\end{frame}

\begin{frame}{Texto Aqui}
\end{frame}

%-----------------------------------------------------------------------------

\section{Demonstração Prática}

\begin{frame}{Texto Aqui}
\end{frame}

\begin{frame}{Texto Aqui}
\end{frame}

\begin{frame}{Texto Aqui}
\end{frame}

%-----------------------------------------------------------------------------

\section{Conclusões}

\begin{frame}{Conclusões}
\end{frame}

\begin{frame}{Conclusões}
\end{frame}

\begin{frame}{Conclusões}
\end{frame}

%-----------------------------------------------------------------------------

\section{Referências}
\begin{frame}[t]{Referências}
    \begin{itemize}
        \item BREIMAN, L.; FRIEDMAN, J.; STONE, C.J.; OLSHEN, R.A. Classification and Regression Trees. Taylor \& Francis, 1984. ISBN 9780412048418.
    \end{itemize}
\end{frame}

\begin{frame}
    \centering
   Árvores de Regressão: Processo de Treinamento
   \par\noindent\rule{\textwidth}{0.5pt}
    Jean Marcelo Mira Junior\\
    Ramiro Luiz Nunes

\end{frame}

\end{document}

%-----------------------------------------------------------------------------